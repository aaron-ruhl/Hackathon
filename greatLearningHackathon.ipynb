{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaron-ruhl/Hackathon/blob/main/greatLearningHackathon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spDSpEsA_VPp"
      },
      "source": [
        "# Importing libraries, data, and dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3XjXLkTNgW4"
      },
      "source": [
        "## Libraries & Options"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83wTlS2qWxRB"
      },
      "source": [
        "**Libraries**\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GaflJHRcE8O"
      },
      "outputs": [],
      "source": [
        "!pip install feature_engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkuWdle3VNvH"
      },
      "outputs": [],
      "source": [
        "#importing standard python libraries for working with numbers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#incase I decide to make diagnostic plots of skewed distributions\n",
        "import scipy.stats as stats\n",
        "\n",
        "#sklearn libraries for data pre-processing\n",
        "from sklearn.model_selection import train_test_split,StratifiedKFold,cross_val_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import metrics\n",
        "\n",
        "#sklearn libraries for model building\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier,AdaBoostClassifier\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "#library for make_pipeline\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "#sklearn library used for hypertuning\n",
        "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
        "\n",
        "#imblearn library for under/over sampling\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import SMOTENC\n",
        "\n",
        "#feature_engine libraries for refined feature engineering steps\n",
        "from feature_engine.imputation import (\n",
        "    AddMissingIndicator,\n",
        "    RandomSampleImputer\n",
        ")\n",
        "from feature_engine.outliers import ArbitraryOutlierCapper\n",
        "from feature_engine.encoding import OrdinalEncoder,OneHotEncoder\n",
        "import feature_engine.transformation as vt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5mp7CX6IVsS"
      },
      "source": [
        "**Options**\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ul3Zzk3yX__f"
      },
      "outputs": [],
      "source": [
        "#Setting pandas display options\n",
        "pd.options.display.max_columns = 50\n",
        "pd.options.display.max_rows = 100\n",
        "\n",
        "#google colab display options\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5KhzyL2_dm4"
      },
      "source": [
        "## Dataset & Dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMYqiX9hXJY5"
      },
      "source": [
        "**Dataset**\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5B0p4inVjNw"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "uploaded=files.upload()\n",
        "data=pd.read_csv(io.BytesIO(uploaded[\"Train_set.csv\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOqfyCSwXOUp"
      },
      "source": [
        "**Dictionary**\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8ota9wXYBY_"
      },
      "outputs": [],
      "source": [
        "uploaded2=files.upload()\n",
        "dictionary=pd.read_csv(io.BytesIO(uploaded2[\"Data_Dictionary.csv\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_qDy1zh_lgA"
      },
      "source": [
        "# Data Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P3-lvvqYpjW"
      },
      "source": [
        "## Preliminary Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKB1gXZFaHjX"
      },
      "outputs": [],
      "source": [
        "dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Egl7VzztWwVL"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_CFaXqzW6gh"
      },
      "outputs": [],
      "source": [
        "data.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh5tEcpXXOov"
      },
      "source": [
        "*Observations:*\n",
        "- `ID` is unique to everyone and not useful\n",
        "- Categorical Variables\n",
        "  - Ordinal\n",
        "    - `loan_grade`,`loan_subgrade`,\n",
        "  - Nominal\n",
        "    - `loan_term`,`home_ownership`,`income_verification_status`,`loan_purpose`, `state_code`,`application_type`,`job_experience`, & `default`, which is the target class\n",
        "  \n",
        "- Numerical Variables\n",
        "  - Discreet\n",
        "    - `loan_amnt`, `dlinq_2yrs`, `public_records`, `revolving_balance`, `total_acc`, `last_week_pay`\n",
        "  - Continuous\n",
        "    -  `interest_rate`, `annual_income`, `debt_to_income`, `interest_recieve`,`total_current_balance`, `total_revolving_limit`  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Der2U4x_W8Ce"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGFcdR4KW-th"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPqorXOrXAEw"
      },
      "outputs": [],
      "source": [
        "data.iloc[:,1:].describe(include=np.number).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dql5urR2eB15"
      },
      "source": [
        "*Observations:*\n",
        "- `total_current_balance` and `total_revolving_limit` are missing observations in around 5000 rows.\n",
        "- `interest_rate`, `interest_recieve`, `revolving_balance`, & `debt_to_income` have pretty high maximums. `total_current_balance` & `total_revolving_limit` aswell.\n",
        "  - lognormal, univariate distribution is what these appear to describe so far.\n",
        "- Most applicants did not have any legal cases open (`public_records`) or prior 30+ day delinquencies within the last 2 yrs(`delinq_2yrs`).\n",
        "- `total_acc` & `last_week_pay` also have a similar high maximum skew, but these are discreet variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0TZ1balNeRs"
      },
      "outputs": [],
      "source": [
        "high_maximums=['interest_rate', 'interest_recieve', 'revolving_balance', 'debt_to_income','total_current_balance', 'total_revolving_limit']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXKR-yZ8iyxN"
      },
      "outputs": [],
      "source": [
        "data.iloc[:,1:].describe(include=np.object_).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bjMmY-GjUq3"
      },
      "source": [
        "*Observations:*\n",
        "- 7 `loan_grade` and 35 `loan_subgrade`, this will give me an excellent metric for fine tuning the prediciton model; no missing values aswell, which makes sense considering every application gets scored.\n",
        "- `state_code` had 50 different possibilities, which is great, business was booming. Already I can see that 'CA' was present the most with around 13.5 thousand entries recorded.\n",
        "  - The excessive cardinality of `loan_grade/loan_subgrade` & `state_code` might need to be converted into ordinal values that retains the precise ordering for the former and fewer categories that retain the overall hierarchy for the latter.\n",
        "- The rest of the variables are relatively well behaved and can be discussed further in EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynHfJdwEXHZd"
      },
      "outputs": [],
      "source": [
        "data.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fm80SNAgxi_"
      },
      "source": [
        "*Observations:*\n",
        "- `job_experience`, `total_current_balance`, `total_revolving_balance`, `deling_2yrs`, `annual_income`, & `total_acc` seem like things that an applicant might have left blank either randomly or maybe some reason that is not random. I should impute and add an indicator for each if that row contained a missing value. I might need to get creative and EDA will help with that.\n",
        "  - I wonder if the 2 missing observations for `public_records`, `delinq_2yrs`, & `total_acc` occured with the same `ID`.\n",
        "- `last_week_pay` this might be just representing zero values because the applicant did not choose to pay off some of EMI early. The amount of missing is smaller here and seems to have been a rare occurence. Adding and indicator here might be a good idea; I will know more after EDA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRzBK6zgXI_Y"
      },
      "outputs": [],
      "source": [
        "data.default.value_counts(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk35Dx1LpJFg"
      },
      "source": [
        "*Observation:*\n",
        "- Considering that there is almost one hundred thousand observations, this is imbalanced, but not terrible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqFpRnHC8D9Y"
      },
      "source": [
        "# Splitting & Isolating Data \\*run before proceeding\\*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N9H41eZ_pXx"
      },
      "source": [
        "### Splitting data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ntnP_wrpWZo"
      },
      "outputs": [],
      "source": [
        "X=data.drop('default',axis=1)\n",
        "y=data['default'].astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmBktN21_vQo"
      },
      "source": [
        "**Importing X_test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P61rphec8juY"
      },
      "outputs": [],
      "source": [
        "uploadedTest = files.upload()\n",
        "testData=pd.read_csv(io.BytesIO(uploadedTest['Test_set.csv']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kquTjS1k6h-8"
      },
      "source": [
        "**making the split of train data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuY_BZyJ8j8A"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X,y,random_state=10,test_size=0.25,stratify=y)\n",
        "X_train.shape, X_val.shape\n",
        "\n",
        "X_test = testData.copy() #The actual 'y_test' is not included and is checked on leaderboard for hackathon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMMVm2UXpDxl"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na0P58YV_0ZP"
      },
      "source": [
        "### Building data_EDA with ID's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AkEc87U9LCl"
      },
      "outputs": [],
      "source": [
        "# I can use the ID's for EDA to avoid data leakage by only looking at X_train in EDA.\n",
        "print('Same amount of unique rows as the entire X_train dataset proof:',X_train.shape,'\\n',\n",
        "      X_train.ID.unique,sep='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gszDRAQNADfT"
      },
      "outputs": [],
      "source": [
        "#Lets check to see that it worked\n",
        "X_train_IDs = list(X_train.ID.unique())\n",
        "len(X_train_IDs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63sviuOoAUmf"
      },
      "outputs": [],
      "source": [
        "X_train_IDs[:30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcKWYQKwFnyC"
      },
      "outputs": [],
      "source": [
        "data_EDA = data.set_index('ID')\n",
        "data_EDA = data_EDA.loc[X_train_IDs]\n",
        "data_EDA.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxknMgl_IvNS"
      },
      "outputs": [],
      "source": [
        "#just a couple sanity checks\n",
        "(data_EDA.index == X_train_IDs).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcDoJ1stJnyy"
      },
      "outputs": [],
      "source": [
        "data_EDA.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EH9XM7zxJsvZ"
      },
      "outputs": [],
      "source": [
        "X_train.head(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ezbKGYgJ0Rx"
      },
      "source": [
        "*Observations:*\n",
        "- Data appears to have remained intact. Note, I already split it up and the steps above only required that I get the ID's from X_train."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "ZIToI5kNjUXM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtixprUGB7Ur"
      },
      "source": [
        "### --- Establishing EDA Standards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLYleu4Qb1bI"
      },
      "source": [
        "1) Categorical\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MskVaaX1_5Xn"
      },
      "outputs": [],
      "source": [
        "#I am setting the columns as a list of like-terms for easier graphing. Also it helps enhance reproducibility during model building, which can easily start to get frustrating when multiple changes need to be made.\n",
        "\n",
        "ordinal = ['loan_grade','loan_subgrade']\n",
        "nominal = ['loan_term','home_ownership','income_verification_status','loan_purpose', 'state_code','application_type','job_experience']\n",
        "\n",
        "target='default'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGCBF-wPb76Z"
      },
      "source": [
        "2) Numerical\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zk1JKoI8E6iG"
      },
      "outputs": [],
      "source": [
        "discreet = ['loan_amnt', 'delinq_2yrs', 'public_records', 'revolving_balance', 'total_acc', 'last_week_pay']\n",
        "continuous = ['interest_rate', 'annual_income', 'debt_to_income', 'interest_receive','total_current_balance', 'total_revolving_limit']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo9u8FwcEciu"
      },
      "source": [
        "## **EDA --- Categorical**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6hQlx2f1gH2"
      },
      "source": [
        "## **Univariate**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHg5Ym9iqSOa"
      },
      "source": [
        "### --- labeled_barplot\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hL5M8f-k6MfX"
      },
      "outputs": [],
      "source": [
        "def labeled_barplot(data, feature, perc=False, n=None):\n",
        "    \"\"\"\n",
        "    Barplot with percentage at the top\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    perc: whether to display percentages instead of count (default is False)\n",
        "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
        "    \"\"\"\n",
        "\n",
        "    total = len(data[feature])  # length of the column\n",
        "    count = data[feature].nunique()\n",
        "    if n is None:\n",
        "        plt.figure(figsize=(count + 2, 6))\n",
        "    else:\n",
        "        plt.figure(figsize=(n + 2, 6))\n",
        "\n",
        "    plt.xticks(rotation=90, fontsize=15)\n",
        "    ax = sns.countplot(\n",
        "        data=data,\n",
        "        x=feature,\n",
        "        palette=\"Paired\",\n",
        "        order=data[feature].value_counts().index[:n],\n",
        "    )\n",
        "\n",
        "    for p in ax.patches:\n",
        "        if perc == True:\n",
        "            label = \"{:.1f}%\".format(\n",
        "                100 * p.get_height() / total\n",
        "            )  # percentage of each class of the category\n",
        "        else:\n",
        "            label = p.get_height()  # count of each level of the category\n",
        "\n",
        "        x = p.get_x() + p.get_width() / 2  # width of the plot\n",
        "        y = p.get_height()  # height of the plot\n",
        "\n",
        "        ax.annotate(\n",
        "            label,\n",
        "            (x, y),\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            size=12,\n",
        "            xytext=(0, 5),\n",
        "            textcoords=\"offset points\",\n",
        "        )  # annotate the percentage\n",
        "\n",
        "    plt.show()  # show the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT7jIjCGDOCu"
      },
      "source": [
        "### **Ordinal Only**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hg5285al6SVj"
      },
      "outputs": [],
      "source": [
        "for feature in ordinal:\n",
        "  if (data_EDA[feature].isna()).any() == True:\n",
        "    print(\"*\"*80,\"\\n\",\n",
        "          \"missing detected for {}, and total missing: {}\".format(feature,data_EDA[feature].isna().sum()))\n",
        "  labeled_barplot(data_EDA,feature,target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP9d2aFc1kO4"
      },
      "source": [
        "*Observations:*\n",
        "- Most of the loan applications were given a `loan_grade` of \"B\" or \"C\", followed by 'A\" or D. Then E, F, and G\n",
        "- `loan_subgrade` contains a reasonable sample of all types of subgrade."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35lFZKuQ1fJC"
      },
      "source": [
        "### **Nominal Only**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwXzkmpO9bYW"
      },
      "outputs": [],
      "source": [
        "for feature in nominal:\n",
        "  if (data_EDA[feature].isna()).any() == True:\n",
        "    print(\"*\"*80,\"\\n\",\n",
        "          \"missing detected for {}, and total missing: {}\".format(feature,data_EDA[feature].isna().sum()))\n",
        "  labeled_barplot(data_EDA,feature,target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRacnmLH7_1l"
      },
      "source": [
        "*Observations:*\n",
        "\n",
        "`loan_term`  \n",
        "  - Around 2x \"3 years\"(48970) compared to \"5 years\"(20910)\n",
        "\n",
        "`home_ownership`\n",
        "  - \"Mortgage\"/\"Rent\"(34738/28175) is 50/50 vast majority minus \"OWN\", which takes up the rest(6945).\n",
        "\n",
        "`income_verification_status`\n",
        "  - Relatively even split of \"Source Verified\", \"Verified\", and \"Not Verified\".\n",
        "\n",
        "`loan_purpose`\n",
        "  - \"Debt consolidation\" listed most often as the reason for getting the loan.\n",
        "  - \"home_improvement\" and \"credit_card\" were seperated from \"other\".\n",
        "\n",
        "`state_code`\n",
        "  - \"Idaho\" only has one observation & this could cause dimensionality issues if I just convert it into OHE like this.\n",
        "  - Consider how `loan_grade` and `loan_subgrade` are seperated into A,B,C,D and a1,a2,a3,...,g5. We can see how the latter allows much more precision, while the former offers reduced complexity/variance. In fact I will decide later on to just remove the main grading because it adds too much bias.\n",
        "    - The states might be organized into some reasonable grouping that retains the overall demographical differences. I should try to keep as much precision as possible whilst reducing the overall complexity here.\n",
        "\n",
        "`application_type`\n",
        "  - Only 45 observations for \"JOINT\", which technically should be 90 people, but much less than the almost 70k in \"INDIVIDUAL\". I will consider removing this column to avoid biasing the \"INDIVIDUAL\" applicants.\n",
        "\n",
        "`job_experience`\n",
        "  - I might be able to use random imputation with missing indicator variables to great effect here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TxDb4JkD3gh"
      },
      "source": [
        "## **Target**\n",
        "****\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W46uYok_Zxmm"
      },
      "source": [
        "### --- stacked_barplot\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqLrFLIu99CD"
      },
      "outputs": [],
      "source": [
        "def stacked_barplot(data, feature, target):\n",
        "    \"\"\"\n",
        "    Print the category counts and plot a stacked bar chart\n",
        "\n",
        "    data: dataframe\n",
        "    feature: independent variable\n",
        "    target: target variable\n",
        "    \"\"\"\n",
        "    count = data[feature].nunique()\n",
        "    sorter = data[target].value_counts().index[-1]\n",
        "\n",
        "    tab1 = pd.crosstab(data[feature], data[target], margins=True).sort_values(\n",
        "        by=sorter, ascending=False\n",
        "    )\n",
        "    sorter2 = data[target].value_counts(1).index[-1]\n",
        "    tab2 = pd.crosstab(data[feature], data[target], margins=True, normalize='index').sort_values(\n",
        "        by=sorter2, ascending=False\n",
        "    )\n",
        "    print(\"-\" * 120)\n",
        "    print(tab1)\n",
        "    print(\"-\" * 120)\n",
        "    print(tab2)\n",
        "\n",
        "    tab = pd.crosstab(data[feature], data[target], normalize=\"index\").sort_values(\n",
        "        by=sorter, ascending=False\n",
        "    )\n",
        "    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 1, 5))\n",
        "    plt.legend(\n",
        "        loc=\"lower left\",\n",
        "        frameon=False,\n",
        "    )\n",
        "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmAlvHldBVGR"
      },
      "source": [
        "### **Ordinal Only**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rx6USj4XD9Km"
      },
      "outputs": [],
      "source": [
        "for feature in ordinal:\n",
        "  if (data_EDA[feature].isna()).any() == True:\n",
        "    print(\"*\"*80,\"\\n\",\n",
        "          \"missing detected for {}, and total missing: {}\".format(feature,data_EDA[feature].isna().sum()))\n",
        "  stacked_barplot(data_EDA,feature,target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH3IEa3EGXo0"
      },
      "source": [
        "*Observations:*\n",
        "- `loan_grade` & `loan_subgrade` can be replaced with just one or the other because they both include A,B,C,D distinctions.\n",
        "  - I just need to add and ordinal value for the subgrade and indicators for the overall grade so the classification model may \"see\" this interaction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOy5jkOBBcE6"
      },
      "source": [
        "### **Nominal Only**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyzJEb_hEMZR"
      },
      "outputs": [],
      "source": [
        "for feature in nominal:\n",
        "  if (data_EDA[feature].isna()).any() == True:\n",
        "    print(\"*\"*80,\"\\n\",\n",
        "          \"missing detected for {}, and total missing: {}\".format(feature,data_EDA[feature].isna().sum()))\n",
        "  stacked_barplot(data_EDA,feature,target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBfeR2fiUBHn"
      },
      "source": [
        "*Observations:*\n",
        "- `state_code` might be further seperated based on south, east, north, and west. Plus others"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smK2p3iAEU1E"
      },
      "source": [
        "## **EDA --- Numerical**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8tNqn3WamY7"
      },
      "source": [
        "## **Univariate**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHDju5YeFl8z"
      },
      "source": [
        "### --- histogram_boxplot\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRAzOvKDMWq6"
      },
      "outputs": [],
      "source": [
        "def histogram_boxplot(data, feature, figsize=(12, 7), kde=False,bins=None,hue=None,color=None,palette=None):\n",
        "    \"\"\"\n",
        "    Boxplot and histogram combined\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    figsize: size of figure (default (12,7))\n",
        "    kde: whether to the show density curve (default False)\n",
        "    bins: number of bins for histogram (default None)\n",
        "    \"\"\"\n",
        "    f2, (ax_box2, ax_box3, ax_hist2) = plt.subplots(\n",
        "        nrows=3,  # Number of rows of the subplot grid= 2\n",
        "        sharex=True,  # x-axis will be shared among all subplots\n",
        "        gridspec_kw={\"height_ratios\": (0.20, 0.20, 0.60)},\n",
        "        figsize=figsize,\n",
        "    )  # creating the 2 subplots\n",
        "    sns.boxplot(\n",
        "        data=data, x=feature, ax=ax_box2, showmeans=True, color='RebeccaPurple'\n",
        "    )  # boxplot will be created and a star will indicate the mean value of the column\n",
        "    if hue == None:\n",
        "      sns.violinplot(\n",
        "          data=data, x=feature, ax=ax_box3, palette=palette\n",
        "      )\n",
        "    if hue != None:\n",
        "      sns.boxplot(\n",
        "          data=data, x=feature, y=data[hue], ax=ax_box3, showmeans=True, palette=palette, orient=\"h\"\n",
        "    )\n",
        "    sns.histplot(\n",
        "        data=data, x=feature, kde=kde, hue=hue, ax=ax_hist2, bins=bins, palette=palette\n",
        "    ) if bins else sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2\n",
        "    )  # For histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
        "    )  # Add mean to the histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
        "    )  # Add median to the histogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF3GfSySFLXo"
      },
      "source": [
        "### **Discreet**\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpnWQyup-Je2"
      },
      "outputs": [],
      "source": [
        "for feature in discreet:\n",
        "  if (data_EDA[feature].isna()).any() == True:\n",
        "    print(\"*\"*80,\"\\n\",\n",
        "          \"missing detected for {}, and total missing: {}\".format(feature,data_EDA[feature].isna().sum()))\n",
        "  histogram_boxplot(data_EDA,feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJifFlgTOeCU"
      },
      "source": [
        "*Observations:*\n",
        "- Need to check values of `delinq_2yrs` & `public_records` greater than 2-5\n",
        "- Should investigate `revolving_balance` > 60k\n",
        "- Should check `total_acc` > 60\n",
        "- `last_week_pay` > 250"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mNANTm-Pr0E"
      },
      "outputs": [],
      "source": [
        "data_EDA[data_EDA['delinq_2yrs']>17]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoqL9uiqQpwD"
      },
      "outputs": [],
      "source": [
        "data_EDA[data_EDA['public_records']>12]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sf47FSAHQpXu"
      },
      "outputs": [],
      "source": [
        "data_EDA[data_EDA['revolving_balance']>900000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kn4LZux5Qqid"
      },
      "outputs": [],
      "source": [
        "data_EDA[data_EDA['total_acc']>100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnU-7g3PQqmg"
      },
      "outputs": [],
      "source": [
        "data_EDA[data_EDA['last_week_pay']>269]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTA-h96cFuV_"
      },
      "source": [
        "### **Continuous**\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2AlxKucDF8b"
      },
      "outputs": [],
      "source": [
        "for feature in continuous:\n",
        "  if (data_EDA[feature].isna()).any() == True:\n",
        "    print(\"*\"*80,\"\\n\",\n",
        "          \"missing detected for {}, and total missing: {}\".format(feature,data_EDA[feature].isna().sum()))\n",
        "  histogram_boxplot(data_EDA,feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI0YuCf2R8Ps"
      },
      "outputs": [],
      "source": [
        "data_EDA[data_EDA['annual_income']>2000000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYf6X5YDR8Ps"
      },
      "outputs": [],
      "source": [
        "data_EDA[data_EDA['debt_to_income']>55.0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVJuSElcR8Ps"
      },
      "outputs": [],
      "source": [
        "data_EDA[data_EDA['total_current_balance']>3000000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzpPUZhPR8Ps"
      },
      "outputs": [],
      "source": [
        "data_EDA[data_EDA['total_revolving_limit']>1000000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe1xVcZTaz4f"
      },
      "source": [
        "## **Target**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkIVR62Za4-b"
      },
      "source": [
        "### **Discreet**\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bhbFYiUDtFv"
      },
      "outputs": [],
      "source": [
        "for feature in discreet:\n",
        "  if (data_EDA[feature].isna()).any() == True:\n",
        "    print(\"*\"*80,\"\\n\",\n",
        "          \"missing detected for {}, and total missing: {}\".format(feature,data_EDA[feature].isna().sum()))\n",
        "  histogram_boxplot(data_EDA,feature,hue=target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFSgfJnVa8gH"
      },
      "source": [
        "### Continuous\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSE9zW4RDq-M"
      },
      "outputs": [],
      "source": [
        "for feature in continuous:\n",
        "  if (data_EDA[feature].isna()).any() == True:\n",
        "    print(\"*\"*80,\"\\n\",\n",
        "          \"missing detected for {}, and total missing: {}\".format(feature,data_EDA[feature].isna().sum()))\n",
        "  histogram_boxplot(data_EDA,feature,hue=target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b9qbgpWFGPO"
      },
      "source": [
        "## **EDA --- Target Only**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tzWe_mHFIWd"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "sns.histplot(data_EDA,x=target);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_yavtm7GKC-"
      },
      "source": [
        "*Observation:*\n",
        "- imbalanced, but still pretty decent amount of the minority class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EmEqWBMFaNM"
      },
      "source": [
        "## **EDA --- Correlation Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0ohfPYNFb_k"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,14))\n",
        "sns.heatmap(data_EDA.corr(),annot=True,cmap=\"Spectral\");"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "vYJxwIUYjqDY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTo4mEeR_DYN"
      },
      "source": [
        "## Establishing preprocessing standards \\*Run before proceeding\\*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHgmyb4V-ctq"
      },
      "source": [
        "Grouping `state_code` based on well established demographic maps maintained by the 'US Census Bureau'. There is actually a further grouping that reduces cardinality even more.\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02Avf_nEnNta"
      },
      "outputs": [],
      "source": [
        "newEngland = ['CT','ME','MA','NH','RI','VT']\n",
        "middleAtlantic = ['NJ','NY','PA']\n",
        "eastNorthCentral = ['IN','IL','MI','OH','WI']\n",
        "westNorthCentral = ['IA','KS','MN','MO','NE','ND','SD']\n",
        "southAtlantic = ['DE','DC','FL','GA','MD','NC','SC','VA','WV']\n",
        "eastSouthCentral = ['AL','KY','MS','TN']\n",
        "westSouthCentral = ['AR','LA','OK','TX']\n",
        "mountain = ['AZ','CO','ID','NM','MT','UT','NV','WY']\n",
        "pacific = ['AK','CA','HI','OR','WA']\n",
        "\n",
        "myList= [newEngland,middleAtlantic,eastNorthCentral,westNorthCentral,southAtlantic,eastSouthCentral,westSouthCentral,mountain,pacific]\n",
        "myNames= ['newEngland','middleAtlantic','eastNorthCentral','westNorthCentral','southAtlantic','eastsouthCentral','westSouthCentral','mountain','pacific']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYWxB-y9Sx_b"
      },
      "outputs": [],
      "source": [
        "def state_code_filter(data):\n",
        "  i=0\n",
        "\n",
        "  for y in myList:\n",
        "    for x in y:\n",
        "      data.loc[(data['state_code']==x),'state_code'] = myNames[i]\n",
        "    i+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlnnhtcoC_nr"
      },
      "source": [
        "Defining the Outlier Capping\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ttycq6BwDBMB"
      },
      "outputs": [],
      "source": [
        "capper = ArbitraryOutlierCapper(max_capping_dict={\n",
        "    'delinq_2yrs': 18, 'public_records': 13, 'revolving_balance': 1000000,'total_acc': 100,'last_week_pay': 270,'annual_income': 2000000,'debt_to_income': 55.0,'total_current_balance': 3000000,'total_revolving_limit': 1000000\n",
        "    },\n",
        "                                min_capping_dict=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igL88-gxCXd8"
      },
      "source": [
        "Missing\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7MGAMGDDP5n"
      },
      "outputs": [],
      "source": [
        "missing=['job_experience','annual_income','delinq_2yrs','public_records','total_acc','last_week_pay','total_current_balance','total_revolving_limit','interest_receive','debt_to_income']\n",
        "#data_EDA.isna().sum()>0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numerical Columns + `loan_subgrade`\n",
        "****"
      ],
      "metadata": {
        "id": "dcoWHSn4MzVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numericalColumns = discreet+continuous\n",
        "numericalColumns.append('loan_subgrade')\n",
        "numericalColumns"
      ],
      "metadata": {
        "id": "SqAYMcHKM6c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKmlr_S--S10"
      },
      "source": [
        "## Experimentation with data_EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FStoJJaz9utD"
      },
      "source": [
        "### --- diagnostic_plots\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aC-2bx5IoBx"
      },
      "outputs": [],
      "source": [
        "def diagnostic_plots(df, feature, bins=28):\n",
        "    # The function takes a dataframe (df) and\n",
        "    # the feature of interest as arguments.\n",
        "\n",
        "    # Define figure size.\n",
        "    plt.figure(figsize=(16, 4))\n",
        "\n",
        "    # histogram\n",
        "    plt.subplot(1, 3, 1)\n",
        "    sns.histplot(df[feature], bins=bins)\n",
        "    plt.title('Histogram')\n",
        "\n",
        "    # Q-Q plot\n",
        "    plt.subplot(1, 3, 2)\n",
        "    stats.probplot(df[feature], dist=\"norm\", plot=plt)\n",
        "    plt.ylabel('Case Status')\n",
        "\n",
        "    # boxplot\n",
        "    plt.subplot(1, 3, 3)\n",
        "    sns.boxplot(y=df[feature])\n",
        "    plt.title('Boxplot')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQobR-5NYWgw"
      },
      "source": [
        "### Filtering Categories with data_EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ffr54E4NUvaC"
      },
      "outputs": [],
      "source": [
        "state_code_filter(data_EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxeyJbJFxLP6",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "data_EDA.state_code.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRU50WPJJ5WC"
      },
      "source": [
        "*Observations:*\n",
        "- This is a huge improvement!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JazQ6ELGkRY"
      },
      "source": [
        "### Imputing Missing Values with data_EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FC7Ms8u6wBIY"
      },
      "outputs": [],
      "source": [
        "data_EDA.replace({'interest_receive':0},value=np.nan,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JVXB6haAjnH"
      },
      "outputs": [],
      "source": [
        "missingIndicator = AddMissingIndicator()\n",
        "data_EDA = missingIndicator.fit_transform(data_EDA)\n",
        "\n",
        "randomImputer = RandomSampleImputer(random_state = 1, variables=missing)\n",
        "data_EDA = randomImputer.fit_transform(data_EDA)\n",
        "\n",
        "print(data_EDA.isna().sum())\n",
        "\n",
        "data_EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVPQJ3Jqt36L"
      },
      "outputs": [],
      "source": [
        "data_EDA.describe(include=np.number).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kFqnAOrGx9F"
      },
      "source": [
        "### Capping Outliers with data_EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_LbIIWSIRkL"
      },
      "outputs": [],
      "source": [
        "# outlier detection using boxplot\n",
        "numeric_columns = discreet + continuous\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 12))\n",
        "\n",
        "for i, feature in enumerate(numeric_columns):\n",
        "    plt.subplot(6, 4, i + 1)\n",
        "    plt.boxplot(data_EDA[feature], whis=1.5)\n",
        "    plt.tight_layout()\n",
        "    plt.title(feature)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZv-BWDrdmcT"
      },
      "outputs": [],
      "source": [
        "data_EDA = capper.fit_transform(data_EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-rnLNLGDcTT"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Some checks to make sure it is working as expected\n",
        "'''\n",
        "\n",
        "#data_EDA[data_EDA['delinq_2yrs'] >= 18]\n",
        "#data_EDA[data_EDA['public_records'] >= 13]\n",
        "#data_EDA[data_EDA['revolving_balance'] >= 1000000]\n",
        "#data_EDA[data_EDA['total_acc'] >= 100]\n",
        "#data_EDA[data_EDA['last_week_pay'] >= 270]\n",
        "#data_EDA[data_EDA['annual_income'] >= 2000000]\n",
        "#data_EDA[data_EDA['debt_to_income'] >= 55.0]\n",
        "#data_EDA[data_EDA['total_current_balance'] >= 3000000]\n",
        "#data_EDA[data_EDA['total_revolving_limit'] >= 1000000]\n",
        "#data_EDA.info()\n",
        "data_EDA.describe(include=np.number).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kigqwBxuNTE"
      },
      "outputs": [],
      "source": [
        "data.describe(include=np.number).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtQz08G_uUNe"
      },
      "source": [
        "**Massive improvement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWqHE2ZrGAdC"
      },
      "source": [
        "### Checking transforms with data_EDA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr2DDfrlGcjl"
      },
      "source": [
        "**Discreet**\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OozkDsGzI-1-"
      },
      "outputs": [],
      "source": [
        "for feature in discreet:\n",
        "  if (data_EDA[feature].isna()).any() == True:\n",
        "    print(\"*\"*80,\"\\n\",\n",
        "          \"missing detected for {}, and total missing: {}\".format(feature,data_EDA[feature].isna().sum()))\n",
        "  diagnostic_plots(data_EDA,feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtKKYoShXnEj"
      },
      "source": [
        "---\n",
        "log transform\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhF7qw4iXnEj"
      },
      "outputs": [],
      "source": [
        "for feature in continuous:\n",
        "  if (data_EDA[feature].isna()).any() == True:\n",
        "    print(\"*\"*80,\"\\n\",\n",
        "          \"missing detected for {}, and total missing: {}\".format(feature,data_EDA[feature].isna().sum()))\n",
        "\n",
        "  transform=pd.DataFrame(np.log(data_EDA[feature]))\n",
        "\n",
        "  diagnostic_plots(transform,feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-ZTnd1tXnEk"
      },
      "source": [
        "---\n",
        "sqrt transform\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJDYXLkWXnEk"
      },
      "outputs": [],
      "source": [
        "for feature in continuous:\n",
        "  if (data_EDA[feature].isna()).any() == True:\n",
        "    print(\"*\"*80,\"\\n\",\n",
        "          \"missing detected for {}, and total missing: {}\".format(feature,data_EDA[feature].isna().sum()))\n",
        "\n",
        "  transform=pd.DataFrame((data_EDA[feature])**1/2)\n",
        "\n",
        "  diagnostic_plots(transform,feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiQYib6UXnEk"
      },
      "source": [
        "---\n",
        "yeojohnson\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K72YajhvXnEk"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import yeojohnson\n",
        "\n",
        "for feature in continuous:\n",
        "  if (data_EDA[feature].isna()).any() == True:\n",
        "    print(\"*\"*80,\"\\n\",\n",
        "          \"missing detected for {}, and total missing: {}\".format(feature,data_EDA[feature].isna().sum()))\n",
        "\n",
        "  transform,_= stats.yeojohnson(data_EDA[feature])\n",
        "  boxcox_EDA = pd.DataFrame(transform,columns=[feature])\n",
        "  boxcox_EDA\n",
        "\n",
        "  diagnostic_plots(boxcox_EDA,feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz8ihXXYGef6"
      },
      "source": [
        "**Continuous**\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GHofXu7kIpf-"
      },
      "outputs": [],
      "source": [
        "for feature in continuous:\n",
        "  if (data_EDA[feature].isna()).any():\n",
        "    print(\"*\"*80,\"\\n\",\n",
        "          \"missing detected for {}, and total missing: {}\".format(feature,data_EDA[feature].isna().sum()))\n",
        "  diagnostic_plots(data_EDA,feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnuUhONJTIRh"
      },
      "source": [
        "---\n",
        "log transform\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPNkTAgtWdx5"
      },
      "outputs": [],
      "source": [
        "for feature in continuous:\n",
        "  if (data_EDA[feature].isna()).any() == True:\n",
        "    print(\"*\"*80,\"\\n\",\n",
        "          \"missing detected for {}, and total missing: {}\".format(feature,data_EDA[feature].isna().sum()))\n",
        "\n",
        "  transform=pd.DataFrame(np.log(data_EDA[feature]))\n",
        "\n",
        "  diagnostic_plots(transform,feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqvGCBzBTNLy"
      },
      "source": [
        "---\n",
        "sqrt transform\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_qXokf6Wv0v"
      },
      "outputs": [],
      "source": [
        "for feature in continuous:\n",
        "  if (data_EDA[feature].isna()).any() == True:\n",
        "    print(\"*\"*80,\"\\n\",\n",
        "          \"missing detected for {}, and total missing: {}\".format(feature,data_EDA[feature].isna().sum()))\n",
        "\n",
        "  transform=pd.DataFrame((data_EDA[feature])**1/2)\n",
        "\n",
        "  diagnostic_plots(transform,feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDVSIbyUTmya"
      },
      "source": [
        "---\n",
        "yeojohnson\n",
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mc9n5U3LW6qk"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import yeojohnson\n",
        "\n",
        "for feature in continuous:\n",
        "  if (data_EDA[feature].isna()).any() == True:\n",
        "    print(\"*\"*80,\"\\n\",\n",
        "          \"missing detected for {}, and total missing: {}\".format(feature,data_EDA[feature].isna().sum()))\n",
        "\n",
        "  transform,_= stats.yeojohnson(data_EDA[feature])\n",
        "  boxcox_EDA = pd.DataFrame(transform,columns=[feature])\n",
        "  boxcox_EDA\n",
        "\n",
        "  diagnostic_plots(boxcox_EDA,feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### StandardScaler with data_EDA"
      ],
      "metadata": {
        "id": "b0loWTTUFCI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "continuous"
      ],
      "metadata": {
        "id": "o0PXqJsMIG11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scaling the numerical columns\n",
        "scaler = StandardScaler()\n",
        "\n",
        "data_EDA[continuous] = scaler.fit_transform(\n",
        "    data_EDA[continuous])"
      ],
      "metadata": {
        "id": "aGP9fRZsFSpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler.get_feature_names_out()"
      ],
      "metadata": {
        "id": "NaMQfD6OFSpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_EDA[continuous].describe().T"
      ],
      "metadata": {
        "id": "gxUfoN0OIMh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eCF_MWPCVvN"
      },
      "source": [
        "### Trying Ordinal Discretization with data_EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5cs5YXAFW9l"
      },
      "outputs": [],
      "source": [
        "data_EDA.public_records.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jaYmZdJE0G4"
      },
      "outputs": [],
      "source": [
        "data_EDA.replace({'public_records': {\n",
        "    0: \"Zero\", 1.0: \"One\", 2.0: \"Two\", 3.0: \"Three\", 4.0: \"Four\", 5.0: \"Five\", 5.0: \"Five\", 6.0: \"Six\",\n",
        "    7.0: \"Ten_to_Thirteen\", 8.0: \"Ten_to_Thirteen\", 9.0: \"Ten_to_Thirteen\", 10.0: \"Ten_to_Thirteen\", 11.0: \"Ten_to_Thirteen\", 12.0: \"Ten_to_Thirteen\",\n",
        "    13.0: \"Ten_to_Thirteen\"}},inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKBRdzJvF8Q2"
      },
      "outputs": [],
      "source": [
        "labeled_barplot(data_EDA,'public_records')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zif1PSy-GOHe"
      },
      "outputs": [],
      "source": [
        "stacked_barplot(data_EDA,'public_records','default')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JhR437CpGvM_"
      },
      "outputs": [],
      "source": [
        "stacked_barplot(data_EDA,'loan_subgrade','default')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rA94eTdfdXoO"
      },
      "outputs": [],
      "source": [
        "data_EDA_Xtrain = data_EDA.drop('default',axis=1)\n",
        "data_EDA_ytrain = data_EDA['default']\n",
        "\n",
        "enc = OrdinalEncoder(encoding_method = 'ordered',variables=['loan_subgrade','public_records'])\n",
        "\n",
        "data_EDA_Xtrain = enc.fit_transform(data_EDA_Xtrain, data_EDA_ytrain)\n",
        "\n",
        "data_EDA_Xtrain.loan_subgrade.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGzrNdboh_2m"
      },
      "outputs": [],
      "source": [
        "enc.encoder_dict_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frnForcWiNvA"
      },
      "outputs": [],
      "source": [
        "data_EDA_Xtrain.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcYwNTu2KGwq"
      },
      "source": [
        "## Preparing Data \\*Can skip if going to \"Final Pipeline\"\\*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQq1SEyrGYRG"
      },
      "source": [
        "**Final Steps for pipeline**\n",
        "****\n",
        "These are the final steps I decided based on experimentation and can be skipped if just running the final pipeline, but should be ran to use \"Model Building\" section.\n",
        "****\n",
        "\\*\\*NOTE: **ONLY** data_EDA has been altered and manipulated until now. This has ensured X_train,X_val, & X_test were isolated from all experimentation done. Now, for the first time I will access and manipulate these splits.\\*\\*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "warbYjlimXUp"
      },
      "outputs": [],
      "source": [
        "#Removing `loan_grade` because `loan_subgrade` also includes markers for loan_grade.\n",
        "X_train.drop('loan_grade',axis=1,inplace=True)\n",
        "X_val.drop('loan_grade',axis=1,inplace=True)\n",
        "X_test.drop('loan_grade',axis=1,inplace=True)\n",
        "\n",
        "#Removing ID because it is unique and I decided it may not be worth the hassle of grouping in any meaningful way\n",
        "X_train.drop('ID',axis=1,inplace=True)\n",
        "X_val.drop('ID',axis=1,inplace=True)\n",
        "X_test.drop('ID',axis=1,inplace=True)\n",
        "\n",
        "#Fixing one of the values in `job_experience`. It had a '<' character that caused an issue with sklearn\n",
        "X_train.replace({'job_experience': '<5 Years'},value='under5yrs',inplace=True)\n",
        "X_val.replace({'job_experience': '<5 Years'},value='under5yrs',inplace=True)\n",
        "X_test.replace({'job_experience': '<5 Years'},value='under5yrs',inplace=True)\n",
        "\n",
        "#Replacing these zeros with missing values\n",
        "X_train.replace({'interest_receive':0,'total_revolving_limit':0},value=np.nan,inplace=True)\n",
        "X_val.replace({'interest_receive':0,'total_revolving_limit':0},value=np.nan,inplace=True)\n",
        "X_test.replace({'interest_receive':0,'total_revolving_limit':0},value=np.nan,inplace=True)\n",
        "\n",
        "#Running the state_code filter\n",
        "state_code_filter(X_train)\n",
        "state_code_filter(X_val)\n",
        "state_code_filter(X_test)\n",
        "'''\n",
        "A check to run if interested\n",
        "'''\n",
        "#X_train.state_code.value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_1ZVeOKU96K"
      },
      "outputs": [],
      "source": [
        "#Adding missing indicators\n",
        "missingIndicator = AddMissingIndicator(variables=['job_experience','last_week_pay','total_current_balance','total_revolving_limit'])\n",
        "X_train = missingIndicator.fit_transform(X_train)\n",
        "X_val = missingIndicator.transform(X_val)\n",
        "\n",
        "#Random Sample Imputation\n",
        "randomImputer = RandomSampleImputer(random_state = 1, variables=missing)\n",
        "X_train = randomImputer.fit_transform(X_train)\n",
        "X_val = randomImputer.transform(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqTmcQ4aVs5f"
      },
      "outputs": [],
      "source": [
        "#Capping the outliers with capper defined in preprocessing standards\n",
        "X_train = capper.fit_transform(X_train)\n",
        "X_val = capper.transform(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBebFKeqknfa"
      },
      "outputs": [],
      "source": [
        "#Ordinal Encoding of 'loan_subgrade'\n",
        "enc = OrdinalEncoder(encoding_method = 'ordered',variables=['loan_subgrade'])\n",
        "\n",
        "X_train = enc.fit_transform(X_train, y_train)\n",
        "X_val = enc.transform(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDvmFhs4j_B3"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Can run this to get the dictionary from the transformer fit\n",
        "'''\n",
        "#enc.encoder_dict_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSHtsfnp48i4"
      },
      "outputs": [],
      "source": [
        "#Setting the standard for which columns I want to be converted to OHE\n",
        "oneHotCols = ['loan_term','home_ownership','income_verification_status',\n",
        "              'loan_purpose','state_code','application_type','job_experience'] #state_code should be filtered BEFORE going into OHE. Also notice that 'loan_subgrade' is not included because I was ablw to make this an ordinal variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNdib9VdzDga"
      },
      "outputs": [],
      "source": [
        "#Making OHE columns to prepare categorical data for sklearn\n",
        "X_train=pd.get_dummies(X_train, columns=oneHotCols)\n",
        "X_val=pd.get_dummies(X_val, columns=oneHotCols)\n",
        "X_test=pd.get_dummies(X_test, columns=oneHotCols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3FTt8SsyfzN"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Final check to see what this ended up producing\n",
        "'''\n",
        "\n",
        "#X_train.describe(include=np.number).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FL9xEkoRrgS4"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Final check to see what this ended up producing\n",
        "'''\n",
        "\n",
        "#X_train.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOFilmLqGgwq"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Scorer\n",
        "****"
      ],
      "metadata": {
        "id": "5RohsR98hKTc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUUxQhepoLOb"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "# Type of scoring used to compare parameter combinations\n",
        "scorer = metrics.make_scorer(metrics.accuracy_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Confusion Matrix\n",
        "****"
      ],
      "metadata": {
        "id": "2ymE9zdShTgc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qjmd-4Da6PFT"
      },
      "outputs": [],
      "source": [
        "## Function to create confusion matrix\n",
        "def make_confusion_matrix(model,y_actual,X_test=X_val,labels=[1, 0]): #I exposed X_test in the function definition so I can switch it to X_val if needed\n",
        "    '''\n",
        "    model : classifier to predict values of X\n",
        "    y_actual : ground truth\n",
        "\n",
        "    '''\n",
        "    y_predict = model.predict(X_test)\n",
        "    cm=metrics.confusion_matrix( y_actual, y_predict, labels=[0, 1])\n",
        "    df_cm = pd.DataFrame(cm, index = [i for i in [\"Actual - No\",\"Actual - Yes\"]],\n",
        "                  columns = [i for i in ['Predicted - No','Predicted - Yes']])\n",
        "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                cm.flatten()]\n",
        "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                         cm.flatten()/np.sum(cm)]\n",
        "    labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n",
        "              zip(group_counts,group_percentages)]\n",
        "    labels = np.asarray(labels).reshape(2,2)\n",
        "    plt.figure(figsize = (10,7))\n",
        "    sns.heatmap(df_cm, annot=labels,fmt='')\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2UCNl503L1M"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### --- Defining repeatedly used variables\n",
        "****"
      ],
      "metadata": {
        "id": "yXmulgISdkDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting default data length, number of splits equal to 5, and building a list used repeatedly\n",
        "dataLength = len(X_train)\n",
        "n_splits=5\n",
        "fold_columns=[\"fold1\",\"fold2\",\"fold3\",\"fold4\",\"fold5\"]"
      ],
      "metadata": {
        "id": "Mz2HUqrSdkie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqCDCbcw4jas"
      },
      "source": [
        "### Model Building with Original data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kSuznKd4Rh3"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Setting up the models list\n",
        "'''\n",
        "\n",
        "models = []  # Empty list to store all the models\n",
        "\n",
        "# Appending models into the list\n",
        "\n",
        "models.append((\"dtree\", DecisionTreeClassifier(random_state=7)))\n",
        "models.append((\"logit\", LogisticRegression(random_state=7)))\n",
        "models.append((\"bagging\", BaggingClassifier(random_state=7)))\n",
        "models.append((\"random_forest\", RandomForestClassifier(random_state=7)))\n",
        "models.append((\"adaboost\", AdaBoostClassifier(random_state=7)))\n",
        "models.append((\"gradient\", GradientBoostingClassifier(random_state=7)))\n",
        "models.append((\"xgboost\", XGBClassifier(random_state=7)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sX7mqHLok4Sv"
      },
      "outputs": [],
      "source": [
        "results = []  # Empty list to store all model's CV scores\n",
        "names = []  # Empty list to store name of the models\n",
        "\n",
        "print(\"\\n\",\n",
        "      \"running cross-validation on training dataset, {} splits & length of {}...\\n\".format(n_splits,len(X_train))\n",
        "      )\n",
        "\n",
        "# loop through all models to get the mean cross validated score\n",
        "for name, model in models:\n",
        "    kfold = StratifiedKFold(\n",
        "        n_splits=n_splits, shuffle=True, random_state=1\n",
        "    )\n",
        "    cv_result = cross_val_score(\n",
        "        estimator=model, X=X_train, y=y_train, scoring=scorer, cv=kfold\n",
        "    )\n",
        "    results.append(cv_result)\n",
        "    names.append(name)\n",
        "#making a DataFrame of the results for graphing\n",
        "results_plot=(pd.DataFrame(results,columns=fold_columns,index=names)).T\n",
        "\n",
        "print(results_plot,'\\n\\nMean cross-validation scores...\\n\\n',\n",
        "      results_plot.mean(),sep='')\n",
        "\n",
        "print(\"\\n\" \"checking performance against `X_val` dataset...\")\n",
        "\n",
        "\n",
        "scores = []\n",
        "\n",
        "# loop through all models to get the validation data score\n",
        "for name, model in models:\n",
        "    model.fit(X_train, y_train)\n",
        "    score = metrics.accuracy_score(y_val, model.predict(X_val))\n",
        "    scores.append(score)\n",
        "\n",
        "results_val_plot = pd.DataFrame(scores,index=names,columns=[\"Accuracy\"])\n",
        "results_val_plot #making a DataFrame of the results for graphing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bucf5eXdk7y4"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "sns.boxplot(data=results_plot,showmeans=True)\n",
        "plt.title(\"Accuracy scores for non-linear models\")\n",
        "plt.ylabel(\"Accuracy\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRp__5Xqk-6A"
      },
      "outputs": [],
      "source": [
        "sns.barplot(data=results_val_plot,x=results_val_plot[\"Accuracy\"],\n",
        "            y=results_val_plot.index)\n",
        "plt.yticks(rotation=45)\n",
        "plt.title(\"Test of models against `X_val` dataset\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dySzud_rk-6A"
      },
      "outputs": [],
      "source": [
        "for model in models:\n",
        "  print(model[0],\"*\"*50)\n",
        "  model=model[1].fit(X_train,y_train)\n",
        "  make_confusion_matrix(model,y_train,X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTCLI8KHk-6A"
      },
      "outputs": [],
      "source": [
        "for model in models:\n",
        "  print(model[0],\"*\"*50)\n",
        "  model=model[1].fit(X_train,y_train)\n",
        "  make_confusion_matrix(model,y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aimb6bn4jat"
      },
      "source": [
        "### Model Building with Undersampled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhxfTkvu4jat"
      },
      "outputs": [],
      "source": [
        "# Random undersampler for under sampling the data\n",
        "rus = RandomUnderSampler(random_state=1, sampling_strategy=1)\n",
        "X_train_un, y_train_un = rus.fit_resample(X_train, y_train)\n",
        "print('Original y_train:\\n {}\\nNew y_train_un: \\n{}'.format(y_train.value_counts(1),y_train_un.value_counts(1)),'\\n',sep='')\n",
        "X_train_un.shape,y_train_un.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgLoEy1Tis8h"
      },
      "outputs": [],
      "source": [
        "results_un = []  # Empty list to store all model's CV scores\n",
        "names_un = []\n",
        "\n",
        "dataLengthUn = len(X_train_un)\n",
        "\n",
        "print(\"\\n\",\n",
        "      \"running cross-validation on training dataset, {} splits & length of {}...which is a test size of {} and train size of {}\\n\".format(n_splits,dataLengthUn,np.round(dataLengthUn/5,0),np.round(dataLengthUn-(dataLengthUn/5),0))\n",
        "      )\n",
        "\n",
        "# loop through all models to get the mean cross validated score\n",
        "for name, model in models:\n",
        "    kfold = StratifiedKFold(\n",
        "        n_splits=n_splits, shuffle=True, random_state=1\n",
        "    )\n",
        "    cv_result = cross_val_score(\n",
        "        estimator=model, X=X_train_un, y=y_train_un, scoring=scorer, cv=kfold\n",
        "    )\n",
        "    results_un.append(cv_result)\n",
        "    names_un.append(name)\n",
        "\n",
        "#making a DataFrame of the results for graphing\n",
        "results_plot_un=(pd.DataFrame(results_un,columns=fold_columns,index=names_un)).T\n",
        "\n",
        "print(results_plot_un,'\\n\\nMean cross-validation scores...\\n',\n",
        "      results_plot_un.mean(),sep='')\n",
        "\n",
        "print(\"\\n\" \"checking performance against `X_val` dataset...\")\n",
        "\n",
        "scores=[]\n",
        "\n",
        "# loop through all models to get the validation data score\n",
        "for name, model in models:\n",
        "    model.fit(X_train_un, y_train_un)\n",
        "    score = metrics.accuracy_score(y_val, model.predict(X_val))\n",
        "    scores.append(score)\n",
        "\n",
        "results_val_plot_un = pd.DataFrame(scores,index=names_un,columns=[\"Accuracy\"])\n",
        "results_val_plot_un #making a DataFrame of the results for graphing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5cQFxZ3FzhD"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "sns.boxplot(data=results_plot_un,showmeans=True)\n",
        "plt.title(\"Accuracy scores for non-linear models\")\n",
        "plt.ylabel(\"Accuracy\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_sWN9oalwpm"
      },
      "outputs": [],
      "source": [
        "sns.barplot(data=results_val_plot_un,x=results_val_plot_un[\"Accuracy\"],\n",
        "            y=results_val_plot_un.index)\n",
        "plt.yticks(rotation=45)\n",
        "plt.title(\"Test of models against `X_val` dataset\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81gDYk1S7OHe"
      },
      "outputs": [],
      "source": [
        "for model in models:\n",
        "  print(model[0],\"*\"*50)\n",
        "  model=model[1].fit(X_train_un,y_train_un)\n",
        "  make_confusion_matrix(model,y_train_un,X_train_un)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIvYRTC7c-NE"
      },
      "outputs": [],
      "source": [
        "for model in models:\n",
        "  print(model[0],\"*\"*50)\n",
        "  model=model[1].fit(X_train_un,y_train_un)\n",
        "  make_confusion_matrix(model,y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBKJaFU24jas"
      },
      "source": [
        "### Model Building with Oversampled data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKxnygkE4jat"
      },
      "outputs": [],
      "source": [
        "# Synthetic Minority Over Sampling Technique\n",
        "smnc = SMOTENC(sampling_strategy=1, k_neighbors=5, random_state=1, categorical_features=np.arange(13,45))\n",
        "\n",
        "X_train_over, y_train_over = smnc.fit_resample(X_train, y_train)\n",
        "\n",
        "print('Original y_train:\\n{}\\nNew y_train_over: \\n{}'.format(y_train.value_counts(1),y_train_over.value_counts(1)),'\\n',sep='')\n",
        "X_train_over.shape,y_train_over.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYDlbnUO4jat"
      },
      "outputs": [],
      "source": [
        "results_over = []  # Empty list to store all model's CV scores\n",
        "names_over = []\n",
        "\n",
        "dataLengthOver = len(X_train_over)\n",
        "\n",
        "print(\"\\n\",\n",
        "      \"running cross-validation on training dataset, {} splits & length of {}...which is a test size of {} and train size of {}\\n\".format(n_splits,dataLengthOver,np.round(dataLengthOver/5,0),np.round(dataLengthOver-(dataLengthOver/5),0))\n",
        "      )\n",
        "\n",
        "# loop through all models to get the mean cross validated score\n",
        "for name, model in models:\n",
        "    kfold = StratifiedKFold(\n",
        "        n_splits=n_splits, shuffle=True, random_state=1\n",
        "    )\n",
        "    cv_result = cross_val_score(\n",
        "        estimator=model, X=X_train_over, y=y_train_over, scoring=scorer, cv=kfold\n",
        "    )\n",
        "    results_over.append(cv_result)\n",
        "    names_over.append(name)\n",
        "\n",
        "#making a DataFrame of the results for graphing\n",
        "results_plot_over=(pd.DataFrame(results_over,columns=fold_columns,index=names_over)).T\n",
        "\n",
        "print(results_plot_over,'\\n\\nMean cross-validation scores...\\n\\n',\n",
        "      results_plot_over.mean(),sep='')\n",
        "\n",
        "print(\"\\n\" \"checking performance against `X_val` dataset...\")\n",
        "\n",
        "scores=[]\n",
        "\n",
        "# loop through all models to get the validation data score\n",
        "for name, model in models:\n",
        "    model.fit(X_train_over, y_train_over)\n",
        "    score = metrics.accuracy_score(y_val, model.predict(X_val))\n",
        "    scores.append(score)\n",
        "\n",
        "results_val_plot_over = pd.DataFrame(scores,index=names_over,columns=[\"Accuracy\"])\n",
        "results_val_plot_over #making a DataFrame of the results for graphing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0gESdetF0tP"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "sns.boxplot(data=results_plot_over,showmeans=True)\n",
        "plt.title(\"Accuracy scores for non-linear models\")\n",
        "plt.ylabel(\"Accuracy\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z97Soj3DljSW"
      },
      "outputs": [],
      "source": [
        "sns.barplot(data=results_val_plot_over,x=results_val_plot_over[\"Accuracy\"],\n",
        "            y=results_val_plot_over.index)\n",
        "plt.yticks(rotation=45)\n",
        "plt.title(\"Test of models against `X_val` dataset\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdGhA8Pn6blp"
      },
      "outputs": [],
      "source": [
        "for model in models:\n",
        "  print(model[0],\"*\"*50)\n",
        "  model=model[1].fit(X_train_over,y_train_over)\n",
        "  make_confusion_matrix(model,y_train_over,X_train_over)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rcrl37lc3-O"
      },
      "outputs": [],
      "source": [
        "for model in models:\n",
        "  print(model[0],\"*\"*50)\n",
        "  model=model[1].fit(X_train_over,y_train_over)\n",
        "  make_confusion_matrix(model,y_val,X_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI218eJnnsEG"
      },
      "source": [
        "### Model Building with Under/Oversampled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v20ggeTFn2qO"
      },
      "outputs": [],
      "source": [
        "# Random undersampler for under sampling the data\n",
        "rus = RandomUnderSampler(random_state=1, sampling_strategy=.5)\n",
        "X_train_un2, y_train_un2 = rus.fit_resample(X_train, y_train)\n",
        "print('Original y_train:\\n {}\\nNew y_train_un2: \\n{}'.format(y_train.value_counts(1),y_train_un2.value_counts(1)),'\\n',sep='')\n",
        "X_train_un2.shape,y_train_un2.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPCYfJn4nsEG"
      },
      "outputs": [],
      "source": [
        "# Synthetic Minority Over Sampling Technique\n",
        "smnc = SMOTENC(sampling_strategy=1, k_neighbors=5, random_state=1, categorical_features=np.arange(13,45))\n",
        "\n",
        "X_train_over2, y_train_over2 = smnc.fit_resample(X_train_un2, y_train_un2)\n",
        "\n",
        "print('Original y_train:\\n{}\\nNew y_train_over: \\n{}'.format(y_train.value_counts(1),y_train_over2.value_counts(1)),'\\n',sep='')\n",
        "X_train_over2.shape,y_train_over2.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mN8PwyqgnsEH"
      },
      "outputs": [],
      "source": [
        "results_over2 = []  # Empty list to store all model's CV scores\n",
        "names_over2 = []\n",
        "\n",
        "n_splits=5# Setting number of splits equal to 5\n",
        "fold_columns=[\"fold1\",\"fold2\",\"fold3\",\"fold4\",\"fold5\"]\n",
        "dataLengthOverUn = len(X_train_over2)\n",
        "\n",
        "print(\"\\n\",\n",
        "      \"running cross-validation on training dataset, {} splits & length of {}...which is a test size of {} and train size of {}\\n\".format(n_splits,dataLengthOverUn,np.round(dataLengthOverUn/5,0),np.round(dataLengthOverUn-(dataLengthOverUn/5),0))\n",
        "      )\n",
        "\n",
        "# loop through all models to get the mean cross validated score\n",
        "for name, model in models:\n",
        "    kfold = StratifiedKFold(\n",
        "        n_splits=n_splits, shuffle=True, random_state=1\n",
        "    )\n",
        "    cv_result = cross_val_score(\n",
        "        estimator=model, X=X_train_over2, y=y_train_over2, scoring=scorer, cv=kfold\n",
        "    )\n",
        "    results_over2.append(cv_result)\n",
        "    names_over2.append(name)\n",
        "\n",
        "#making a DataFrame of the results for graphing\n",
        "results_plot_over2=(pd.DataFrame(results_over2,columns=fold_columns,index=names_over2)).T\n",
        "\n",
        "print(results_plot_over2,'\\n\\nMean cross-validation scores...\\n\\n',\n",
        "      results_plot_over2.mean(),sep='')\n",
        "\n",
        "print(\"\\n\" \"checking performance against `X_val` dataset...\")\n",
        "\n",
        "scores=[]\n",
        "\n",
        "# loop through all models to get the validation data score\n",
        "for name, model in models:\n",
        "    model.fit(X_train_over2, y_train_over2)\n",
        "    score = metrics.accuracy_score(y_val, model.predict(X_val))\n",
        "    scores.append(score)\n",
        "\n",
        "results_val_plot_over2 = pd.DataFrame(scores,index=names_over2,columns=[\"Accuracy\"])\n",
        "results_val_plot_over2 #making a DataFrame of the results for graphing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_r4Jtc0nsEH"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "sns.boxplot(data=results_plot_over2,showmeans=True)\n",
        "plt.title(\"Accuracy scores for non-linear models\")\n",
        "plt.ylabel(\"Accuracy\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfPtJZtjnsEI"
      },
      "outputs": [],
      "source": [
        "sns.barplot(data=results_val_plot_over2,x=results_val_plot_over2[\"Accuracy\"],\n",
        "            y=results_val_plot_over2.index)\n",
        "plt.yticks(rotation=45)\n",
        "plt.title(\"Test of models against `X_val` dataset\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UV9Pqr_ynsEI"
      },
      "outputs": [],
      "source": [
        "for model in models:\n",
        "  print(model[0],\"*\"*50)\n",
        "  model=model[1].fit(X_train_over2,y_train_over2)\n",
        "  make_confusion_matrix(model,y_train_over2,X_train_over2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBapH3qQnsEI"
      },
      "outputs": [],
      "source": [
        "for model in models:\n",
        "  print(model[0],\"*\"*50)\n",
        "  model=model[1].fit(X_train_over2,y_train_over2)\n",
        "  make_confusion_matrix(model,y_val,X_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HE7Zzrtd3Xdb"
      },
      "source": [
        "## Hypertuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMReRXdH_YUd"
      },
      "source": [
        "### Tuned with Original data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5LPPDgYWUut"
      },
      "outputs": [],
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# let's check the VIF of the predictors\n",
        "vif_series = pd.Series(\n",
        "    [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])],\n",
        "    index=X_train.columns,\n",
        "    dtype=float,\n",
        ")\n",
        "print(\"VIF values: \\n\\n{}\\n\".format(vif_series))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LI2Nzd5JMIud"
      },
      "outputs": [],
      "source": [
        "models_tuned = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHkVL63-D6D6"
      },
      "source": [
        "`Decision Tree Tuning`\n",
        "****"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pre-Pruning**"
      ],
      "metadata": {
        "id": "c9Tqxh84bzCa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyOabuuufXEI"
      },
      "outputs": [],
      "source": [
        "# defining model\n",
        "Model = DecisionTreeClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid = {'max_depth': np.arange(2,10),\n",
        "              'min_samples_leaf': [1, 4, 7, 10],\n",
        "              'max_leaf_nodes' : [10,12,16,18],\n",
        "              'min_impurity_decrease': [0.0001,0.001,.01] }\n",
        "print(\"Running cross-validation on training dataset, {} splits & length of {}...which is a test size of {} and train size of {}\\n\".format(n_splits,dataLength,np.round(dataLength/5,0),np.round(dataLength-(dataLength/5),0))\n",
        "      )\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=30, n_jobs = -1, scoring=scorer, cv=n_splits, random_state=7,verbose=2)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(X_train,y_train)\n",
        "best_params=randomized_cv.best_params_\n",
        "models_tuned.append((\"dtree_tuned_original\", randomized_cv.best_estimator_))\n",
        "\n",
        "#Printing the results\n",
        "print('Computed final model from CV of various random arrangements created in random grid search....\\n',\n",
        "      '\\n','Best parameters are {}'.format(best_params),\n",
        "      '\\n','---& with CV score(Accuracy)={}'.format(randomized_cv.best_score_),\n",
        "      '\\n','Feature Importances:{}'.format(pd.DataFrame(randomized_cv.best_estimator_.feature_importances_,\n",
        "                                                     index=[X_train.columns],columns=['feature_importances']).sort_values(\n",
        "                                                         by='feature_importances',ascending=False).head(10)\n",
        "                                                     ),sep='')\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGTCiW9FEoDX"
      },
      "source": [
        "`Bagging Classifier Tuning`\n",
        "****"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RANDOM GRID SEARCH**"
      ],
      "metadata": {
        "id": "QzwQDZmVbdDz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZV25fDp366p"
      },
      "outputs": [],
      "source": [
        "# defining model\n",
        "Model = BaggingClassifier(random_state=7)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid = {'max_samples': [0.6,0.9,1],\n",
        "              'max_features': [0.3,0.6,0.9,1],\n",
        "              'n_estimators' : np.arange(50,90,3)\n",
        "}\n",
        "print(\"Running cross-validation on training dataset, {} splits & length of {}...which is a test size of {} and train size of {}\\n\".format(n_splits,dataLength,np.round(dataLength/5,0),np.round(dataLength-(dataLength/5),0))\n",
        "      )\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=30, n_jobs = -1, scoring=scorer, cv=n_splits, random_state=7,verbose=2)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(X_train,y_train)\n",
        "best_params=randomized_cv.best_params_\n",
        "models_tuned.append((\"bagging_tuned_original\", randomized_cv.best_estimator_))\n",
        "\n",
        "#Printing the results\n",
        "print(\"Computed final model from CV of various random arrangements created in random grid search....\\n\\nBest parameters are {}\\n ----& with CV score(Accuracy)={}\\n\".format(best_params,randomized_cv.best_score_))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GRID SEARCH**"
      ],
      "metadata": {
        "id": "G1Nopd5fbgd2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VkgOo_TepBs"
      },
      "outputs": [],
      "source": [
        "# defining model\n",
        "Model = BaggingClassifier(random_state=7)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid = {'max_samples': [0.6,0.9,1],\n",
        "              'max_features': [0.3,0.6,0.9,1],\n",
        "              'n_estimators' : np.arange(50,90,3)\n",
        "}\n",
        "print(\"Running cross-validation on training dataset, {} splits & length of {}...which is a test size of {} and train size of {}\\n\".format(n_splits,dataLength,np.round(dataLength/5,0),np.round(dataLength-(dataLength/5),0))\n",
        "      )\n",
        "\n",
        "#Calling GridSearchCV\n",
        "grid_cv = GridSearchCV(estimator=Model, param_grid=param_grid, n_jobs = -1, scoring=scorer, cv=n_splits,verbose=2)\n",
        "\n",
        "#Fitting parameters in GridSearchCV\n",
        "grid_cv=grid_cv.fit(X_train,y_train)\n",
        "best_params=grid_cv.best_params_\n",
        "models_tuned.append((\"bagging_tuned_original\", grid_cv.best_estimator_))\n",
        "\n",
        "#Printing the results\n",
        "print(\"Computed final model from CV of various random arrangements created in random grid search....\\n\\nBest parameters are {}\\n ----& with CV score(Accuracy)={}\\n\".format(best_params,grid_cv.best_score_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPVb7UKwD3Lr"
      },
      "source": [
        "`Random Forest Tuning`\n",
        "****"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RANDOM GRID SEARCH**"
      ],
      "metadata": {
        "id": "WvAMxN_JbFX4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsQXzcpqfXS5"
      },
      "outputs": [],
      "source": [
        "# defining model\n",
        "Model = RandomForestClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid = { \"n_estimators\": [100,150,200,250],\n",
        "              \"min_samples_leaf\": np.arange(1,7),\n",
        "              \"max_features\": ['sqrt','log2',None,[0.3,0.2,0.5]],\n",
        "              \"max_samples\": np.arange(0.5, 0.8, 0.1)}\n",
        "\n",
        "print(\"Running cross-validation on training dataset, {} splits & length of {}...which is a test size of {} and train size of {}\\n\".format(n_splits,dataLength,np.round(dataLength/5,0),np.round(dataLength-(dataLength/5),0))\n",
        "      )\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=30, n_jobs = -1, scoring=scorer, cv=n_splits, random_state=7,verbose=2)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(X_train,y_train)\n",
        "best_params=randomized_cv.best_params_\n",
        "models_tuned.append((\"random_forest_tuned_original\", randomized_cv.best_estimator_))\n",
        "\n",
        "#Printing the results\n",
        "print('Computed final model from CV of various random arrangements created in random grid search....\\n',\n",
        "      '\\n','Best parameters are {}'.format(best_params),\n",
        "      '\\n','---& with CV score(Accuracy)={}'.format(randomized_cv.best_score_),\n",
        "      '\\n','Feature Importances:{}'.format(pd.DataFrame(randomized_cv.best_estimator_.feature_importances_,\n",
        "                                                     index=[X_train.columns],columns=['feature_importances']).sort_values(\n",
        "                                                         by='feature_importances',ascending=False).head(10)\n",
        "                                                     ),sep='')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GRID SEARCH**"
      ],
      "metadata": {
        "id": "4YwavUpTbCno"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llDk1ttJtoIw"
      },
      "outputs": [],
      "source": [
        "# defining model\n",
        "Model = RandomForestClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid = { \"n_estimators\": [100,150,250],\n",
        "              \"min_samples_leaf\": np.arange(1,7),\n",
        "              \"max_features\": [[0.3,0.2,0.5],'sqrt'],\n",
        "              \"max_samples\": np.arange(0.5, 0.8, 0.1)}\n",
        "\n",
        "print(\"Running cross-validation on training dataset, {} splits & length of {}...which is a test size of {} and train size of {}\\n\".format(n_splits,dataLength,np.round(dataLength/5,0),np.round(dataLength-(dataLength/5),0))\n",
        "      )\n",
        "\n",
        "#Calling GridSearchCV\n",
        "grid_cv = GridSearchCV(Model, param_grid, n_jobs = -1, scoring=scorer, cv=n_splits,verbose=2)\n",
        "\n",
        "#Fitting parameters in GridSearchCV\n",
        "grid_cv.fit(X_train,y_train)\n",
        "best_params=grid_cv.best_params_\n",
        "\n",
        "#Printing the results\n",
        "print('Computed final model from CV of various random arrangements created in random grid search....\\n',\n",
        "      '\\n','Best parameters are {}'.format(best_params),\n",
        "      '\\n','---& with CV score(Accuracy)={}'.format(grid_cv.best_score_),\n",
        "      '\\n','Feature Importances:{}'.format(pd.DataFrame(grid_cv.best_estimator_.feature_importances_,\n",
        "                                                     index=[X_train.columns],columns=['feature_importances']).sort_values(\n",
        "                                                         by='feature_importances',ascending=False).head(10)\n",
        "                                                     ),sep='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj_Q38vJEQHJ"
      },
      "source": [
        "`XGBoost Tuning`\n",
        "****"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RANDOM GRID SEARCH**"
      ],
      "metadata": {
        "id": "pIG-zOLebLbW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7fUv5Ql1qyO"
      },
      "outputs": [],
      "source": [
        "# defining model\n",
        "Model = XGBClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid={ 'n_estimators': [150,200, 250, 300],\n",
        "            'scale_pos_weight': [5,10,3],\n",
        "            'learning_rate': [0.1,0.2,0.05],\n",
        "            'gamma': [0,3,5],\n",
        "            'subsample': [0.7,0.8,0.9] }\n",
        "\n",
        "print(\"Running cross-validation on training dataset, {} splits & length of {}...which is a test size of {} and train size of {}\\n\".format(n_splits,dataLength,np.round(dataLength/5,0),np.round(dataLength-(dataLength/5),0))\n",
        "      )\n",
        "\n",
        "#Calling RandomizedSearchCV\n",
        "randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=30, n_jobs = -1, scoring=scorer, cv=n_splits, random_state=7,verbose=2)\n",
        "\n",
        "#Fitting parameters in RandomizedSearchCV\n",
        "randomized_cv.fit(X_train,y_train)\n",
        "best_params=randomized_cv.best_params_\n",
        "models_tuned.append((\"xgboost_tuned_original\", randomized_cv.best_estimator_))\n",
        "\n",
        "#Printing the results\n",
        "print('Computed final model from CV of various random arrangements created in random grid search....\\n',\n",
        "      '\\n','Best parameters are {}'.format(best_params),\n",
        "      '\\n','---& with CV score(Accuracy)={}'.format(randomized_cv.best_score_),\n",
        "      '\\n','Feature Importances:{}'.format(pd.DataFrame(randomized_cv.best_estimator_.feature_importances_,\n",
        "                                                     index=[X_train.columns],columns=['feature_importances']).sort_values(\n",
        "                                                         by='feature_importances',ascending=False).head(10)\n",
        "                                                     ),sep='')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GRID SEARCH**"
      ],
      "metadata": {
        "id": "GnjzqaGwbI_I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaVEr0VNnUVg"
      },
      "outputs": [],
      "source": [
        "# defining model\n",
        "Model = XGBClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid to pass in RandomSearchCV\n",
        "param_grid={ 'n_estimators': [150,200,250,300],\n",
        "            'scale_pos_weight': [3,5,10],\n",
        "            'learning_rate': [0.1,0.2,0.05],\n",
        "            'gamma': [0,3,5],\n",
        "            'subsample': [0.7,0.8,0.9] }\n",
        "\n",
        "print(\"Running cross-validation on training dataset, {} splits & length of {}...which is a test size of {} and train size of {}\\n\".format(n_splits,dataLength,np.round(dataLength/5,0),np.round(dataLength-(dataLength/5),0))\n",
        "      )\n",
        "\n",
        "#Calling GridSearchCV\n",
        "grid_cv = GridSearchCV(Model, param_grid,  n_jobs = -1, scoring=scorer, cv=n_splits,verbose=2)\n",
        "\n",
        "#Fitting parameters in GridSearchCV\n",
        "grid_cv.fit(X_train,y_train)\n",
        "best_params=grid_cv.best_params_\n",
        "\n",
        "#Printing the results\n",
        "print('Computed final model from CV of various random arrangements created in random grid search....\\n',\n",
        "      '\\n','Best parameters are {}'.format(best_params),\n",
        "      '\\n','---& with CV score(Accuracy)={}'.format(grid_cv.best_score_),\n",
        "      '\\n','Feature Importances:{}'.format(pd.DataFrame(grid_cv.best_estimator_.feature_importances_,\n",
        "                                                     index=[X_train.columns],columns=['feature_importances']).sort_values(\n",
        "                                                         by='feature_importances',ascending=False).head(10)\n",
        "                                                     ),sep='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq3NisXe3xck"
      },
      "source": [
        "## Model Performance Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl6tDRdoNaHE"
      },
      "source": [
        "### Original data tuning results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiQmnNUJ0j9R"
      },
      "outputs": [],
      "source": [
        "#Save point so I do not have to run all the above tuning results in one long mega-session\n",
        "'''\n",
        "models_tuned = [\n",
        " ('dtree_tuned',\n",
        "  DecisionTreeClassifier(min_samples_leaf= 10, min_impurity_decrease= 0.0001,\n",
        "                            max_leaf_nodes=18, max_depth =9, random_state=7)),\n",
        " ('logit_tuned', LogisticRegression(C=0.1, random_state=7)),\n",
        " ('random_forest_tuned',\n",
        "  RandomForestClassifier(n_estimators= 150, min_samples_leaf= 4, max_samples= 0.7, max_features= None,random_state=7)),\n",
        " ('adaboost_tuned',\n",
        "  AdaBoostClassifier(base_estimator= DecisionTreeClassifier(max_depth=3,\n",
        "                                                            random_state=7),\n",
        "                     n_estimators= 250, learning_rate= 0.2)),\n",
        " ('xgboost_tuned',\n",
        "  XGBClassifier(subsample= 0.9, scale_pos_weight= 3,\n",
        "                      n_estimators = 300, learning_rate = 0.2, gamma = 5,random_state=7)),\n",
        " ('bagging_tuned',\n",
        "  BaggingClassifier(max_features=0.9, max_samples=0.8,\n",
        "                    n_estimators=70,random_state=7))]\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpuwYZCaLAA0"
      },
      "outputs": [],
      "source": [
        "models_tuned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5n95sDejmIQP"
      },
      "outputs": [],
      "source": [
        "results_tuned = []  # Empty list to store all model's CV scores\n",
        "names_tuned = []\n",
        "\n",
        "n_splits=5# Setting number of splits equal to 10\n",
        "fold_columns=[\"fold1\",\"fold2\",\"fold3\",\"fold4\",\"fold5\"]\n",
        "\n",
        "# loop through all models to get the mean cross validated score\n",
        "for name, model in models_tuned:\n",
        "    kfold = StratifiedKFold(\n",
        "        n_splits=n_splits, shuffle=True, random_state=7\n",
        "    )\n",
        "    cv_result = cross_val_score(\n",
        "        estimator=model, X=X_train, y=y_train, scoring=scorer, cv=kfold\n",
        "    )\n",
        "    results_tuned.append(cv_result)\n",
        "    names_tuned.append(name)\n",
        "\n",
        "#making a DataFrame of the results for graphing\n",
        "results_plot_tuned=(pd.DataFrame(results_tuned,columns=fold_columns,index=names_tuned)).T\n",
        "\n",
        "print(results_plot_tuned,'\\n\\nMean cross-validation scores...\\n',\n",
        "      results_plot_tuned.mean(),sep='')\n",
        "\n",
        "print(\"\\n\" \"checking performance against `X_val` dataset...\" \"\\n\")\n",
        "\n",
        "scores=[]\n",
        "\n",
        "# loop through all models to get the validation data score\n",
        "for name, model in models_tuned:\n",
        "    model.fit(X_train, y_train)\n",
        "    score = metrics.accuracy_score(y_val, model.predict(X_val))\n",
        "    scores.append(score)\n",
        "\n",
        "results_val_plot_tuned = pd.DataFrame(scores,index=names_tuned,columns=[\"Accuracy\"])\n",
        "results_val_plot_tuned#making a DataFrame of the results for graphing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM8UqoDcmb_v"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "sns.boxplot(data=results_plot_tuned,showmeans=True)\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Cross Validation Performance of tuned models with original dataset\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFsIdfRkn7nA"
      },
      "outputs": [],
      "source": [
        "sns.barplot(data=results_val_plot_tuned,x=results_val_plot_tuned[\"Accuracy\"],\n",
        "            y=results_val_plot_tuned.index)\n",
        "plt.yticks(rotation=45)\n",
        "plt.xlim(.7,1)\n",
        "plt.title(\"Test of models against `X_val` dataset, tuned with original dataset\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHlND667DeZu"
      },
      "outputs": [],
      "source": [
        "for model in models_tuned:\n",
        "  print(model[0],\"*\"*50)\n",
        "  model=model[1].fit(X_train,y_train)\n",
        "  make_confusion_matrix(model,y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpW1N8FcHFmA"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "final_model = StackingClassifier(\n",
        "    estimators=\n",
        "    [\n",
        "    ('xgboost_tuned',\n",
        "     XGBClassifier(subsample= 0.9, scale_pos_weight= 3,\n",
        "                   n_estimators = 300, learning_rate = 0.2, gamma = 3,random_state=7)),\n",
        "     ('bagging_tuned',\n",
        "      BaggingClassifier(max_features=0.9, max_samples=0.8,\n",
        "                        n_estimators=70,random_state=7)),\n",
        "    ])\n",
        "\n",
        "final_model.fit(X_train, y_train)\n",
        "score = metrics.accuracy_score(y_val, final_model.predict(X_val))\n",
        "\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ll14aWl6QTEG"
      },
      "outputs": [],
      "source": [
        "print(\"Voting Classifier results:\\n\")\n",
        "make_confusion_matrix(final_model,y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgQdwG6XVEm2"
      },
      "source": [
        "# Final Pipeline & Final Results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Final Pipeline"
      ],
      "metadata": {
        "id": "VBgX1cqMi0ED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Establishing Data Filtering"
      ],
      "metadata": {
        "id": "zAaFEoNIi3Jr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\* I am pretty sure I could have defined my own class for the pipeline here, but this works and I prefer making these steps very obvious to see\\*"
      ],
      "metadata": {
        "id": "3kY4O-OPkedg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMEfgPhzXglV"
      },
      "outputs": [],
      "source": [
        "#Removing `loan_grade` because `loan_subgrade` also includes markers for loan_grade.\n",
        "X_train.drop('loan_grade',axis=1,inplace=True)\n",
        "X_val.drop('loan_grade',axis=1,inplace=True)\n",
        "X_test.drop('loan_grade',axis=1,inplace=True)\n",
        "\n",
        "#Removing ID because it is unique and I decided it may not be worth the hassle of grouping in any meaningful way\n",
        "X_train.drop('ID',axis=1,inplace=True)\n",
        "X_val.drop('ID',axis=1,inplace=True)\n",
        "\n",
        "#saving the IDlist so that I can make an excel document with the IDs and results later\n",
        "iDlist = X_test['ID']\n",
        "#then also dropping it as I did with all others\n",
        "X_test.drop('ID',axis=1,inplace=True)\n",
        "\n",
        "#Fixing one of the values in `job_experience`. It had a '<' character that caused an issue with sklearn\n",
        "X_train.replace({'job_experience': '<5 Years'},value='under5yrs',inplace=True)\n",
        "X_val.replace({'job_experience': '<5 Years'},value='under5yrs',inplace=True)\n",
        "X_test.replace({'job_experience': '<5 Years'},value='under5yrs',inplace=True)\n",
        "\n",
        "#Replacing these zeros with missing values\n",
        "X_train.replace({'interest_receive':0,'total_revolving_limit':0},value=np.nan,inplace=True)\n",
        "X_val.replace({'interest_receive':0,'total_revolving_limit':0},value=np.nan,inplace=True)\n",
        "X_test.replace({'interest_receive':0,'total_revolving_limit':0},value=np.nan,inplace=True)\n",
        "\n",
        "#Running the state_code filter\n",
        "state_code_filter(X_train)\n",
        "state_code_filter(X_val)\n",
        "state_code_filter(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ky9ugNXXzRL"
      },
      "outputs": [],
      "source": [
        "#Setting the standard for which columns I want to be converted to OHE 1's or 0s\n",
        "oneHotCols = ['loan_term','home_ownership','income_verification_status',\n",
        "              'loan_purpose','state_code','application_type','job_experience'] #note state_code should already be filtered by this point\n",
        "\n",
        "max_capping_dictionary={\n",
        "        'delinq_2yrs': 18, 'public_records': 13,\n",
        "        'revolving_balance': 1000000,'total_acc': 100,'last_week_pay': 270,\n",
        "        'annual_income': 2000000,'debt_to_income': 55.0,\n",
        "        'total_current_balance': 3000000,'total_revolving_limit': 1000000}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Pipeline"
      ],
      "metadata": {
        "id": "cN7DIH6Di8Fl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdHRt5O9U9gA"
      },
      "outputs": [],
      "source": [
        "#Building a final pipeline whose steps are named automatically using make_pipeline from imblearn.pipeline\n",
        "finalPipe=make_pipeline(\n",
        "    AddMissingIndicator(variables=\n",
        "                        ['job_experience','last_week_pay',\n",
        "                         'total_current_balance','total_revolving_limit']),\n",
        "    RandomSampleImputer(random_state = 1, variables=missing),\n",
        "    ArbitraryOutlierCapper(max_capping_dict=max_capping_dictionary,min_capping_dict=None),\n",
        "    OrdinalEncoder(encoding_method = 'ordered',\n",
        "                   variables=['loan_subgrade']),\n",
        "    OneHotEncoder(variables=oneHotCols),\n",
        "    StackingClassifier(estimators=[\n",
        "        ('xgboost_tuned',\n",
        "     XGBClassifier(subsample= 0.9, scale_pos_weight= 3,\n",
        "                   n_estimators = 300, learning_rate = 0.2, gamma = 3,random_state=7)),\n",
        "        ('bagging_tuned',\n",
        "     BaggingClassifier(max_features= 0.9, max_samples= 0.9, n_estimators= 59))\n",
        "        ],n_jobs=-1)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ytxj7jc-7GN-"
      },
      "outputs": [],
      "source": [
        "finalPipe.steps"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the Final Pipeline"
      ],
      "metadata": {
        "id": "3Y3UM7j7jKAA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_sp9aBmWoFC"
      },
      "outputs": [],
      "source": [
        "finalPipe.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final pipeline: CV results"
      ],
      "metadata": {
        "id": "fzNn7LA3m8Vd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dJiJSier6rK"
      },
      "outputs": [],
      "source": [
        "#Running a CV test with final model\n",
        "n_splits=5\n",
        "fold_columns=[\"fold1\",\"fold2\",\"fold3\",\"fold4\",\"fold5\"]\n",
        "\n",
        "kfold = StratifiedKFold(\n",
        "    n_splits=n_splits, shuffle=True, random_state=7\n",
        ")\n",
        "cv_result = cross_val_score(\n",
        "    estimator=finalPipe, X=X_train, y=y_train, scoring=scorer, cv=kfold\n",
        ")\n",
        "\n",
        "#Printing out the results from CV test\n",
        "print('spread of CV scores: {}\\nMean CV Score: {}'.format(cv_result,cv_result.mean()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQRZMyq0hufl"
      },
      "outputs": [],
      "source": [
        "#Running a CV test with final model\n",
        "n_splits=5\n",
        "fold_columns=[\"fold1\",\"fold2\",\"fold3\",\"fold4\",\"fold5\"]\n",
        "\n",
        "kfold = StratifiedKFold(\n",
        "    n_splits=n_splits, shuffle=True, random_state=7\n",
        ")\n",
        "cv_result = cross_val_score(\n",
        "    estimator=finalPipe, X=X_train, y=y_train, scoring=scorer, cv=kfold\n",
        ")\n",
        "\n",
        "#Printing out the results from CV test\n",
        "print('spread of CV scores: {}\\nMean CV Score: {}'.format(cv_result,cv_result.mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final pipeline: X_val results\n",
        "\n"
      ],
      "metadata": {
        "id": "2e9z2dw1iqbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original run"
      ],
      "metadata": {
        "id": "UogbJHOuuMkr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74YA7HRjr3MU"
      },
      "outputs": [],
      "source": [
        "#Printing out different metrics of X_val test results\n",
        "print('Accuracy: {}\\nRecall: {}\\nPrecision: {}\\nF1: {}'.format(metrics.accuracy_score(finalPipe.predict(X_val),y_val),\n",
        "      metrics.recall_score(finalPipe.predict(X_val),y_val),\n",
        "      metrics.precision_score(finalPipe.predict(X_val),y_val),\n",
        "      metrics.f1_score(finalPipe.predict(X_val),y_val)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ir2Rvi6Ir3MV"
      },
      "outputs": [],
      "source": [
        "#Making a confusion matrix of these results from X_val predictions\n",
        "print(\"Stacking Classifier Results:\\n\")\n",
        "make_confusion_matrix(finalPipe,y_val,X_test=X_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "running a second time in the future with similar results"
      ],
      "metadata": {
        "id": "9kwXz29UuOs6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6SZIYCEhyDc"
      },
      "outputs": [],
      "source": [
        "#Printing out different metrics of X_val test results\n",
        "print('Accuracy: {}\\nRecall: {}\\nPrecision: {}\\nF1: {}'.format(metrics.accuracy_score(finalPipe.predict(X_val),y_val),\n",
        "      metrics.recall_score(finalPipe.predict(X_val),y_val),\n",
        "      metrics.precision_score(finalPipe.predict(X_val),y_val),\n",
        "      metrics.f1_score(finalPipe.predict(X_val),y_val)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzOOF9wMhyDd"
      },
      "outputs": [],
      "source": [
        "#Making a confusion matrix of these results from X_val predictions\n",
        "print(\"Stacking Classifier Results:\\n\")\n",
        "make_confusion_matrix(finalPipe,y_val,X_test=X_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final pipeline: X_test results"
      ],
      "metadata": {
        "id": "DjzQbFGvnBfh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eQdhBxVMq-f"
      },
      "outputs": [],
      "source": [
        "#making results list\n",
        "results = finalPipe.predict(X_test)\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G816_5o6Mq-f"
      },
      "outputs": [],
      "source": [
        "#checking IDlist\n",
        "iDlist"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building csv file**"
      ],
      "metadata": {
        "id": "tVQR53mMnEDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#building DataFrame of \"results\" with IDs as index and default as the only column name\n",
        "finalResults=pd.DataFrame(results,index=iDlist,columns=[\"default\"])\n",
        "finalResults"
      ],
      "metadata": {
        "id": "qPTeTKNEJEQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving this result locally to submit\n",
        "finalResults.to_csv('final_results.csv')\n",
        "\n",
        "files.download(\"final_results.csv\")"
      ],
      "metadata": {
        "id": "A9b1-qm_JiZf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMT2qBTkXGntNEA1CLRcOEr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}